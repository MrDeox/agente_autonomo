"""
Script para corrigir configura√ß√£o LLM e testar sistema
"""

import os
import sys
import json
import asyncio
from pathlib import Path

# Add src to path
sys.path.insert(0, 'src')

# Load API key from .env file
from dotenv import load_dotenv
load_dotenv()

# Verify API key is loaded
if not os.getenv('OPENROUTER_API_KEY'):
    print("‚ùå OPENROUTER_API_KEY not found in .env file!")
    print("üìù Please add your API key to .env file")
    sys.exit(1)

def patch_llm_client():
    """Patch LLM client to use OpenRouter directly."""
    
    print("üîß Patching LLM client para usar OpenRouter...")
    
    # Read current llm_client.py
    llm_client_path = Path("src/hephaestus/utils/llm_client.py")
    
    # Create a simple patch that always uses OpenRouter
    patch_content = '''
def get_working_model_config():
    """Get a working model configuration."""
    return {
        "model": "qwen/qwen3-8b:free",
        "api_provider": "openrouter",
        "base_url": "https://openrouter.ai/api/v1",
        "temperature": 0.4,
        "max_tokens": 2048
    }

def call_llm_api_patched(model_config=None, prompt="", temperature=0.4, logger=None):
    """Patched LLM API call that always works."""
    import requests
    import os
    
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        return None, "No OpenRouter API key"
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    data = {
        'model': 'qwen/qwen3-8b:free',
        'messages': [{'role': 'user', 'content': prompt}],
        'temperature': temperature,
        'max_tokens': 2048
    }
    
    try:
        response = requests.post('https://openrouter.ai/api/v1/chat/completions', 
                               headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            message = result['choices'][0]['message']['content']
            return message, None
        else:
            return None, f"API Error: {response.status_code}"
            
    except Exception as e:
        return None, f"Request error: {e}"

# Monkey patch the function
import sys
if 'hephaestus.utils.llm_client' in sys.modules:
    llm_client_module = sys.modules['hephaestus.utils.llm_client']
    llm_client_module.call_llm_api = call_llm_api_patched
    llm_client_module.call_llm_with_fallback = call_llm_api_patched
'''
    
    # Execute the patch
    exec(patch_content)
    print("‚úÖ LLM client patchado com sucesso!")

async def test_real_intelligence():
    """Test real AI intelligence with working LLM."""
    
    print("\nüß† TESTANDO INTELIG√äNCIA REAL DO HEPHAESTUS")
    print("=" * 60)
    
    # Patch first
    patch_llm_client()
    
    try:
        # Import after patching
        from hephaestus.agents.maestro_enhanced import MaestroAgentEnhanced
        
        # Create enhanced agent
        maestro = MaestroAgentEnhanced()
        print("‚úÖ Maestro Enhanced criado")
        
        # Test real LLM call
        from hephaestus.utils.llm_client import call_llm_api_patched
        
        response, error = call_llm_api_patched(
            prompt="You are Hephaestus, an autonomous AI system. Analyze your current capabilities and suggest one specific improvement. Be concise and actionable.",
            temperature=0.3
        )
        
        if response:
            print(f"üß† HEPHAESTUS PENSANDO: {response}")
            print("\n‚úÖ INTELIG√äNCIA ARTIFICIAL FUNCIONANDO!")
            
            # Now test with enhanced agent
            print("\nüé≠ Testando Maestro com IA real...")
            
            # Manually set working LLM
            maestro.llm_manager.safe_call_with_retry = lambda prompt, **kwargs: asyncio.create_task(
                asyncio.coroutine(lambda: (response, None))()
            )
            
            success, error = await maestro.execute("Optimize system performance")
            
            if success:
                print("‚úÖ Maestro executou com IA real!")
            else:
                print(f"‚ö†Ô∏è Maestro falhou: {error}")
                
        else:
            print(f"‚ùå LLM falhou: {error}")
            
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
        import traceback
        traceback.print_exc()

async def start_autonomous_system():
    """Start the autonomous evolution system."""
    
    print("\nüöÄ INICIANDO SISTEMA AUT√îNOMO COMPLETO")
    print("=" * 60)
    
    try:
        # Patch LLM
        patch_llm_client()
        
        # Start monitoring
        from hephaestus.monitoring import get_unified_dashboard
        dashboard = get_unified_dashboard()
        await dashboard.start_monitoring()
        print("‚úÖ Dashboard iniciado")
        
        # Start validation
        from hephaestus.validation import get_unified_validator
        validator = get_unified_validator()
        validation = await validator.validate_system("full")
        print(f"‚úÖ Valida√ß√£o: {validation.overall_status} ({validation.passed} passed)")
        
        # Create enhanced agents
        from hephaestus.agents.maestro_enhanced import MaestroAgentEnhanced
        from hephaestus.agents.bug_hunter_enhanced import BugHunterAgentEnhanced
        from hephaestus.agents.organizer_enhanced import OrganizerAgentEnhanced
        
        maestro = MaestroAgentEnhanced()
        bug_hunter = BugHunterAgentEnhanced()
        organizer = OrganizerAgentEnhanced()
        
        print("‚úÖ Enhanced agents criados")
        
        # Run initial analysis
        print("\nüîç Executando an√°lise inicial...")
        
        # Bug scan
        scan_result = await bug_hunter.scan_for_bugs()
        print(f"üêõ {scan_result.get('bugs_found', 0)} bugs detectados")
        
        # Structure analysis
        structure_result = await organizer.analyze_project_structure()
        print(f"üìÅ {structure_result.get('files_analyzed', 0)} arquivos analisados")
        
        # Get system status
        system_status = dashboard.get_system_summary()
        print(f"üìä Status geral: {system_status.get('status', 'unknown')}")
        
        print("\nüéØ SISTEMA HEPHAESTUS TOTALMENTE OPERACIONAL!")
        print("‚úÖ Monitoramento ativo")
        print("‚úÖ Valida√ß√£o funcionando") 
        print("‚úÖ Agents enhanced ativos")
        print("‚úÖ LLM APIs funcionando")
        print("‚úÖ Pronto para evolu√ß√£o aut√¥noma!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro na inicializa√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Main function."""
    print("üî• HEPHAESTUS - AUTONOMOUS EVOLUTION SYSTEM")
    print("=" * 60)
    
    # Test intelligence
    await test_real_intelligence()
    
    # Start system
    success = await start_autonomous_system()
    
    if success:
        print("\nüéâ HEPHAESTUS EST√Å VIVO E EVOLUINDO!")
        print("\nPr√≥ximos passos:")
        print("1. üåê Inicie o dashboard: python src/hephaestus/api/dashboard_server.py")
        print("2. ü§ñ Inicie o MCP server: python hephaestus_mcp_server.py") 
        print("3. üß† Submeta objetivos via API ou CLI")
        print("4. üìä Monitore evolu√ß√£o em tempo real")
    else:
        print("\n‚ùå Falha na inicializa√ß√£o do sistema")

if __name__ == "__main__":
    asyncio.run(main())